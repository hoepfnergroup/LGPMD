{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55f9e1d",
   "metadata": {},
   "source": [
    "# Gaussian Process Surrogate Modeling for Molecular Dynamics Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28fec8",
   "metadata": {},
   "source": [
    "## Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e673b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math Packages\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import minimize\n",
    "import time as time\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt  \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# Data saving packages\n",
    "from pickle import dump, load\n",
    "\n",
    "# Parallelization\n",
    "from dask import config as cfg\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "mp.set_start_method('fork')\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" \n",
    "import emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a14a44f-d1d8-47fe-b738-76a9fdcb3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_kernel(x1, x2, l, width):\n",
    "    \"\"\"\n",
    "    Computes the squared exponential kernel between the tensors x and y with hyper-parameters l and width.\n",
    "    N corresponds to the number of samples and D corresponds to the number of dimensions of the input function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    x: Tensor [N,D]\n",
    "        Feature vector for N samples with D dimensions each\n",
    "    \n",
    "    y: Tensor [N,D]\n",
    "        Feature vector for N samples with D dimensions each\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "    \"\"\"\n",
    "    K = width**2 * torch.exp(-(torch.cdist(x1/l,x2/l,p=2)**2)/2)\n",
    "    return K\n",
    "\n",
    "def surrogate(Xi, Xd, l, width, y, KddInv):\n",
    "    \"\"\"\n",
    "    Computes the gaussian process estimate of the structure factor given a set of pair potential parameters. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Xi: Tensor [η*M,D]\n",
    "        Feature vector for M potential samples at η r evaluations with D dimensions each. This \n",
    "        corresponds to the points you wish to infer.\n",
    "    \n",
    "    Xd: Tensor [N,D]\n",
    "        Feature vector for N samples with D dimensions each. This corresponds to the points you trained on.\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter.\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "        \n",
    "    y: Tensor [N,1]\n",
    "        Output feature vector corresponding to the Xd training set.  \n",
    "        \n",
    "    KddInv: Tensor [N,N]\n",
    "        This is the inverted kernel matrix of the training set Xd\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "       \n",
    "    μ: Tensor [η,N]\n",
    "        The mean estimate for each g(r,n,sigma,epsilon) given in Xi. \n",
    "        \n",
    "    \"\"\"\n",
    "    V = torch.stack([((n/(n-6))*((n/6)**((6)/(n-6))))*e*((s/r)**n - (s/r)**6) for n,s,e in zip(Xi[:,0],Xi[:,1],Xi[:,2])])\n",
    "    μ = torch.exp(-V/kbT)\n",
    "    Kid = se_kernel(Xi, Xd, l, width)\n",
    "\n",
    "    return 1 +  (Kid @ KddInv @ (y-1))\n",
    "\n",
    "def local_surrogate(Xi, Xd, l, width, y, KddInv, μd):\n",
    "    \"\"\"\n",
    "    Computes the subset gaussian process estimate of the structure factor given a set of pair potential parameters. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Xi: Tensor [M,D]\n",
    "        Feature vector for M samples with D dimensions each. This corresponds to the points you wish to infer.\n",
    "    \n",
    "    Xd: Tensor [M,D]\n",
    "        Feature vector for N samples with D dimensions each. This corresponds to the points you trained on.\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter.\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "        \n",
    "    KddInv: Tensor [N,N]\n",
    "        This is the inverted kernel matrix of the training set Xd\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    μ: Tensor [η,N]\n",
    "        The mean estimate for each g(r) at each potential. The g(r)'s are organized in vertical lines where the column\n",
    "        dimension indexes the potential parameters. \n",
    "        \n",
    "    \"\"\"\n",
    "    V = torch.stack([((n/(n-6))*((n/6)**((6)/(n-6))))*e*((s/r)**n - (s/r)**6) for n,s,e in zip(Xi[:,0],Xi[:,1],Xi[:,2])])\n",
    "    μ = torch.exp(-V/kbT)\n",
    "    Kid = se_kernel(Xi, Xd, l, width)\n",
    "    return (μ +(Kid @ KddInv) @ (y-μd)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0617c00",
   "metadata": {},
   "source": [
    "## Importing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44418d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training data generated by 1_sample_gen.ipynb\n",
    "\n",
    "rmin = 0.0118331810091873\n",
    "rmax = 15.512161254882812\n",
    "rnum = 73\n",
    "\n",
    "k   = 3.29982916e-27 #boltzmann constant,   [kcal/K/particle]   \n",
    "av  = 6.0223e23      #avagadro number,      [particle/mol]  \n",
    "T   = 42.2           #temperature,          [K]\n",
    "kbT = k * av * T\n",
    "\n",
    "input_dict = load(open('training_data/xs.p', 'rb'))\n",
    "xd = input_dict['xs'].reshape(3*320,3)\n",
    "\n",
    "input_dict = load(open('training_data/rdfs.p', 'rb'))\n",
    "model_rdf = input_dict['rdfs']\n",
    "\n",
    "r = torch.linspace(rmin,rmax,rnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdb81e-7fe3-4578-8c03-1194d778bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentalCSVFilename = 'exp_data/ne_42K_rdf.csv'\n",
    "\n",
    "data = pd.read_csv(experimentalCSVFilename)\n",
    "r_exp_raw = np.array(data['r'])\n",
    "rdf_exp_raw = np.array(data[' g'])\n",
    "\n",
    "rmin = 0.0118331810091873\n",
    "rmax = 15.512161254882812\n",
    "rnum = 73\n",
    "\n",
    "# Interpolalate the experimental data to make it consistent with the simulations\n",
    "r  = torch.tensor(np.linspace(rmin, rmax, num=rnum))\n",
    "rdf_exp_i = interpolate.splrep(r_exp_raw, rdf_exp_raw, s=0)\n",
    "rdf_exp = torch.from_numpy(interpolate.splev(r, rdf_exp_i, der=0))\n",
    "\n",
    "print(\"Old length: \", len(r_exp_raw))\n",
    "print(\"New length: \", len(r))\n",
    "\n",
    "figure(figsize = (12,10),dpi=80)\n",
    "plt.title(\"Experimental\")\n",
    "plt.scatter(r_exp_raw,rdf_exp_raw,alpha=0.4)\n",
    "plt.plot(r, rdf_exp)\n",
    "plt.xlim(rmin,rmax)\n",
    "plt.xlabel(\"$\\AA$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of training set\n",
    "\n",
    "figure(figsize = (12,10),dpi=80)\n",
    "plt.title(\"GP Training Set\")\n",
    "for i in range(len(model_rdf)):\n",
    "    plt.plot(r,model_rdf[i],alpha=0.4)\n",
    "plt.xlim(rmin,rmax)\n",
    "plt.xlabel(\"$\\AA^{-1}$\")\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
    "plt.suptitle('Potential Parameter Distributions')\n",
    "axs[0,0].scatter(xd[:, 0], xd[:, 1],label=\"Samples\")\n",
    "axs[0,0].set_xlabel('n')\n",
    "axs[0,0].set_ylabel('σ')\n",
    "axs[0,1].scatter(xd[:, 0], xd[:, 2],label=\"Samples\")\n",
    "axs[0,1].set_xlabel('n')\n",
    "axs[0,1].set_ylabel('ϵ')\n",
    "axs[1,0].scatter(xd[:, 1], xd[:, 2],label=\"Samples\")\n",
    "axs[1,0].set_xlabel('σ')\n",
    "axs[1,0].set_ylabel('ϵ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbfad6",
   "metadata": {},
   "source": [
    "## Basic Matricies for GP Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a469b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(xd)\n",
    "η = len(r)\n",
    "XdClassic = torch.zeros(n*η,4)\n",
    "yClassic = torch.zeros(n*η)\n",
    "\n",
    "k = 0 # Row index in Xd matrix and y vector.\n",
    "for i in range(n):\n",
    "    for j in range(η):\n",
    "        # Xd_k = (n,σ,ϵ,q)\n",
    "        XdClassic[k] = torch.tensor([xd[i][0],xd[i][1],xd[i][2],r[j]])\n",
    "        yClassic[k] = model_rdf[i][j]\n",
    "        k += 1\n",
    "        \n",
    "yClassic = torch.unsqueeze(yClassic,dim=0).transpose(0,1)\n",
    "\n",
    "# Remake the Xd matrix for the subset matrix \n",
    "Xd = torch.tensor(xd).float()\n",
    "y = model_rdf.float()\n",
    "\n",
    "index = torch.arange(0,len(xd),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab352c1",
   "metadata": {},
   "source": [
    "## Choosing the hyper-parameters based off LOO and LMLH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_i(i, θ):\n",
    "    \n",
    "    l = torch.tensor([θ[0],θ[1],θ[2]]).float()\n",
    "    w = torch.tensor(θ[3]).float()\n",
    "    σn = torch.tensor(θ[4]).float()\n",
    "    \n",
    "    Kdd = se_kernel(Xd,Xd,l,w) + torch.eye(len(Xd))*σn\n",
    "    L = torch.linalg.cholesky(Kdd) # Extract Cholesky decomposition\n",
    "    KddInv = torch.cholesky_inverse(L) \n",
    "    \n",
    "    qN = KddInv @ y.T[i]\n",
    "    \n",
    "    KddInv_ii = torch.diagonal(KddInv, 0)\n",
    "    \n",
    "    qNover       = qN/torch.sqrt(KddInv_ii)\n",
    "    logKddInv_ii = torch.log(KddInv_ii)\n",
    "    \n",
    "    g = (1/(2*len(Xd)))*(qNover@qNover) - (1/(2*len(Xd)))*torch.sum(logKddInv_ii) + (1/2)*np.log(2*np.pi)\n",
    "    \n",
    "    return g\n",
    "\n",
    "def g(θ):\n",
    "    \n",
    "    l = torch.tensor([θ[0],θ[1],θ[2]]).float()\n",
    "    w = torch.tensor(θ[3]).float()\n",
    "    σn = torch.tensor(θ[4]).float()\n",
    "    \n",
    "    Kdd = se_kernel(Xd,Xd,l,w) + torch.eye(len(Xd))*σn\n",
    "    KddInv = torch.linalg.inv(Kdd)\n",
    "    \n",
    "    KddInv_ii = torch.diagonal(KddInv, 0)\n",
    "    \n",
    "    logKddInv_ii = torch.log(KddInv_ii)\n",
    "    \n",
    "    g = (1/(2*len(Xd)))*torch.sum(((KddInv @ y).T/(torch.sqrt(KddInv_ii).repeat(73,1)))**2) - (73/(2*len(Xd)))*torch.sum(logKddInv_ii) + (73/2)*np.log(2*np.pi)\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c201185-6381-429a-928f-94bcafadc802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loo(hyperParamOptions,j_0, j_last):\n",
    "    \n",
    "    looμArr = torch.zeros((j_last - j_0,len(Xd),len(r))) \n",
    "    \n",
    "    for j in range(j_0,j_last,1):\n",
    "\n",
    "        # Calculate Kdd for local GP with hyper parameter index j\n",
    "        arr = hyperParamOptions[j]\n",
    "        l = torch.tensor([arr[0],arr[1],arr[2]]).float()\n",
    "        w = torch.tensor(arr[3]).float()\n",
    "        σn = torch.tensor(arr[4]).float()\n",
    "        Kdd = se_kernel(Xd,Xd,l,w) + torch.eye(len(Xd))*σn\n",
    "    \n",
    "        looμArr_j = torch.zeros(len(Xd),len(r))\n",
    "\n",
    "        Vd = torch.stack([((n/(n-6))*((n/6)**((6)/(n-6))))*e*((s/r)**n - (s/r)**6) for n,s,e in zip(Xd[:,0],Xd[:,1],Xd[:,2])])\n",
    "        μd = torch.exp(-Vd/kbT).float()\n",
    "    \n",
    "        # Leave index i out from training and predict it using Local GP\n",
    "        for i in range(len(Xd)): \n",
    "    \n",
    "            Kdd_i = Kdd[index[index != i]].T[index[index != i]].T\n",
    "            KddInv_i = torch.linalg.inv(Kdd_i)\n",
    "    \n",
    "            # Remove the same values from y\n",
    "            y_i = y[index != i]\n",
    "    \n",
    "            # Again for X data\n",
    "            Xd_i = Xd[index[index != i]]\n",
    "            μd_i = μd[index[index != i]]\n",
    "            Xi = Xd[i].unsqueeze(dim=0)\n",
    "    \n",
    "            # Compute the predictions after leaving one out\n",
    "            looμ = local_surrogate(Xi,Xd_i,l,w,y_i,KddInv_i,μd_i).T\n",
    "            looμArr_j[i] = looμ\n",
    "        \n",
    "        looμArr[j - j_0] = looμArr_j\n",
    "\n",
    "    output_dict = dict(looμArr = looμArr, hyperParamOptions = hyperParamOptions[j_0:j_last])\n",
    "    dump(output_dict, open('training_data/hyperparameter/PMF/hyperParams'+str(j_0)+'.p', 'wb'))\n",
    "\n",
    "    return looμArr, hyperParamOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c893e0-d6a4-4d86-88e3-a93a4eed25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.set({'distributed.scheduler.worker-ttl': None}) # This stops dask from crying when the sims take a long time.\n",
    "client = Client(n_workers=80)\n",
    "# https://ondemand.chpc.utah.edu/rnode/notch149.ipoib.int.chpc.utah.edu/8787/status\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667849c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Attempting to load previously calculated hyper parameters\")\n",
    "    \n",
    "    from pickle import load\n",
    "    input_dict = load(open('training_data/hyperparameterG/PMF/hyperParams.p', 'rb'))\n",
    "    results = input_dict['results']\n",
    "    hyperParamOptions = input_dict['hyperParamOptions']\n",
    "    print(\"Success!!!\")\n",
    "    \n",
    "except:\n",
    "    print(\"Failed\")\n",
    "    \n",
    "    ℓ_nmin = 0.5\n",
    "    ℓ_nmax = 4\n",
    "    \n",
    "    ℓ_σmin = 0.01\n",
    "    ℓ_σmax = 0.05\n",
    "    \n",
    "    ℓ_ϵmin = 0.001\n",
    "    ℓ_ϵmax = 0.01\n",
    "    \n",
    "    w_min = 1e-4\n",
    "    w_max = 1e-1\n",
    "    \n",
    "    σn_min = 1e-4\n",
    "    σn_max = 1e-2\n",
    "    \n",
    "    \n",
    "    # Creates a set of hyper parameters to compare\n",
    "    trials = 5_000\n",
    "    hyperParamOptions = torch.zeros((trials,5))\n",
    "    hyperParamOptions[:,0] = (ℓ_nmax - ℓ_nmin) * torch.rand(trials) + ℓ_nmin\n",
    "    hyperParamOptions[:,1] = (ℓ_σmax - ℓ_σmin) * torch.rand(trials) + ℓ_σmin\n",
    "    hyperParamOptions[:,2] = (ℓ_ϵmax - ℓ_ϵmin) * torch.rand(trials) + ℓ_ϵmin\n",
    "    hyperParamOptions[:,3] = (w_max  - w_min)  * torch.rand(trials) + w_min\n",
    "    hyperParamOptions[:,4] = (σn_max - σn_min) * torch.rand(trials) + σn_min\n",
    "\n",
    "    # Queue up function calls into dask\n",
    "    lazy_results = []\n",
    "    for i in range(trials):\n",
    "        lazy_results.append(dask.delayed(g)(hyperParamOptions[i]))\n",
    "\n",
    "    print(\"Queued Lazy Results\")\n",
    "\n",
    "    results = dask.compute(*lazy_results)\n",
    "        \n",
    "    output_dict = dict(results = results, hyperParamOptions = hyperParamOptions) # logMarginalLHArr = logMarginalLHArr\n",
    "    dump(output_dict, open('training_data/hyperparameterG/PMF/hyperParams.p', 'wb'))\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmin(torch.tensor(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal hyperparameters based on the LOO error')\n",
    "hyperParamOptions[torch.argmin(torch.tensor(results))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5a2a9-df8e-4bab-a6fe-bc5196757380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outdated method\n",
    "# try:\n",
    "#     print(\"Attempting to load previously calculated hyper parameters\")\n",
    "    \n",
    "#     from pickle import load\n",
    "#     input_dict = load(open('training_data/hyperparameter/PMF/hyperParams.p', 'rb'))\n",
    "#     results = input_dict['results']\n",
    "#     hyperParamOptions = input_dict['hyperParamOptions']\n",
    "#     print(\"Success!!!\")\n",
    "    \n",
    "# except:\n",
    "#     print(\"Failed\")\n",
    "    \n",
    "#     # Creates a set of hyper parameters to compare\n",
    "#     hyperParamOptions = torch.zeros((trials,5))\n",
    "#     hyperParamOptions[:,0] = (ℓ_nmax - ℓ_nmin) * torch.rand(trials) + ℓ_nmin\n",
    "#     hyperParamOptions[:,1] = (ℓ_σmax - ℓ_σmin) * torch.rand(trials) + ℓ_σmin\n",
    "#     hyperParamOptions[:,2] = (ℓ_ϵmax - ℓ_ϵmin) * torch.rand(trials) + ℓ_ϵmin\n",
    "#     hyperParamOptions[:,3] = (w_max  - w_min)  * torch.rand(trials) + w_min\n",
    "#     hyperParamOptions[:,4] = (σn_max - σn_min) * torch.rand(trials) + σn_min\n",
    "\n",
    "#     # Storage for the leave one out prediction of local GP\n",
    "#     looμArr = torch.zeros((len(hyperParamOptions),len(Xd),len(r))) \n",
    "    \n",
    "#     # Queue up function calls into dask\n",
    "#     lazy_results = []\n",
    "#     for i in range(int(trials/50)):\n",
    "#         j_0 = (i*50)\n",
    "#         j_last = ((i+1)*50)\n",
    "#         lazy_results.append(dask.delayed(compute_loo)(hyperParamOptions,j_0, j_last))\n",
    "\n",
    "#     print(\"Queued Lazy Results\")\n",
    "\n",
    "#     results = dask.compute(*lazy_results)\n",
    "        \n",
    "#     output_dict = dict(results = results, hyperParamOptions = hyperParamOptions) # logMarginalLHArr = logMarginalLHArr\n",
    "#     dump(output_dict, open('training_data/hyperparameter/PMF/hyperParams.p', 'wb'))\n",
    "    \n",
    "#     print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bba81f-651f-42d2-b1e4-f3f32c45b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outdated method\n",
    "# # Storage for the leave one out prediction of local GP\n",
    "# looμArr = torch.zeros((len(hyperParamOptions),len(Xd),len(r))) \n",
    "\n",
    "# for i in range(int(len(hyperParamOptions)/50)):\n",
    "#     j_0 = (i*50)\n",
    "#     j_last = ((i+1)*50)\n",
    "#     looμArr[j_0:j_last] = results[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af644dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdated method\n",
    "# # Compute the leave one out error for each parameter\n",
    "# LooErr = torch.zeros(5000)\n",
    "# for i in range(len(hyperParamOptions[:5000])):\n",
    "#     LooErr[i] = torch.sum((looμArr[i] - y)**2)\n",
    "# # Grab the one with the minimum error \n",
    "# LooIndex = torch.argmin(LooErr)\n",
    "\n",
    "# print(\"Hyperparameters corresponding to the minimum leave-one-out error: \", hyperParamOptions[LooIndex])\n",
    "# print(\"Average error per training example per point corresponding to the minimum leave-one-out error: \", np.sqrt(LooErr[LooIndex].item()*(1/960)*(1/len(r))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1019ba49-9a26-4a68-b76d-6888fa482d71",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c33ae0-63a0-4e72-aaea-8bfa9cd31981",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsims = 320\n",
    "ndims = 3\n",
    "input_dict = load(open('testing_data/xs_test.p', 'rb'))\n",
    "xs_test = input_dict['xs_test']\n",
    "\n",
    "input_dict = load(open('testing_data/rdfs_test.p', 'rb'))\n",
    "rdfs_test_bruh = input_dict['rdfs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c5bc4-d45c-4f97-95b2-3dbe4b6784ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [3.3110e+00, 4.5928e-02, 9.8081e-03, 9.4022e-02, 7.2281e-04]\n",
    "l = torch.tensor([arr[0],arr[1],arr[2]]).float()\n",
    "w = torch.tensor(arr[3]).float()\n",
    "σn = torch.tensor(arr[4]).float()\n",
    "Kdd = se_kernel(Xd,Xd,l,w) + torch.eye(len(Xd))*σn\n",
    "KddInv = torch.linalg.inv(Kdd)\n",
    "Xi = xs_test.float()\n",
    "\n",
    "Vd = torch.stack([((n/(n-6))*((n/6)**((6)/(n-6))))*e*((s/r)**n - (s/r)**6) for n,s,e in zip(Xd[:,0],Xd[:,1],Xd[:,2])])\n",
    "μd = torch.exp(-Vd/kbT).float() #PMF prior mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430ba6a-8722-431a-ab8c-9a3a5a8e98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPOut = local_surrogate(Xi, Xd, l, w, y, KddInv, μd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430ae27-4cba-42cc-b5ef-9b581a4d26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = torch.sqrt(torch.mean((GPOut.T - rdfs_test_bruh)**2,dim=0))\n",
    "RMSE_total = torch.sqrt(torch.mean((GPOut.T - rdfs_test_bruh)**2))\n",
    "\n",
    "print(np.sqrt(torch.sum(RMSE**2/73)))\n",
    "print(RMSE_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68453083-a7c7-4663-88fc-209e15e1dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = torch.zeros(320)\n",
    "for i in range(320):\n",
    "    err[i] = torch.sqrt(torch.sum((GPOut.T[i] - rdfs_test_bruh[i])**2)/73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93294a57-9563-414f-9a1b-3797ef8e86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 9))\n",
    "axs[0,0].scatter(xs_test[:,0], err, color = 'r', alpha = 0.6, edgecolors = 'k')\n",
    "axs[0,0].hlines(0.03,xmin=min(xs_test[:,0]),xmax=max(xs_test[:,0]),color='k',linestyle='dashed')\n",
    "axs[0,0].set_xlabel('λ', fontsize = 16)\n",
    "axs[0,0].set_ylabel('RMSE', fontsize = 16)\n",
    "#axs[0,0].text(10.8, 0.076, '(a)', fontsize = 16)\n",
    "axs[0,1].hlines(0.03,xmin=min(xs_test[:,1]),xmax=max(xs_test[:,1]),color='k',linestyle='dashed')\n",
    "axs[0,1].scatter(xs_test[:,1], err, color = 'r', alpha = 0.6, edgecolors = 'k')\n",
    "axs[0,1].set_xlabel('σ (Å)', fontsize = 16)\n",
    "axs[0,1].set_ylabel('RMSE', fontsize = 16)\n",
    "#axs[0,1].text(2.92, 0.076, '(b)', fontsize = 16)\n",
    "axs[1,0].scatter(xs_test[:,2], err, color = 'r', alpha = 0.6, edgecolors = 'k')\n",
    "axs[1,0].hlines(0.03,xmin=min(xs_test[:,2]),xmax=max(xs_test[:,2]),color='k',linestyle='dashed')\n",
    "#axs[1,0].vlines(.15,0,0.15,color='g',linestyle='dashed')\n",
    "axs[1,0].set_xlabel('ϵ (kcal/mol)', fontsize = 16)\n",
    "axs[1,0].set_ylabel('RMSE', fontsize = 16)\n",
    "#axs[1,0].text(0.087, 0.076, '(c)', fontsize = 16)\n",
    "axs[1,1].plot(r,RMSE, color = 'k', label = 'Mean', linestyle = '-')\n",
    "axs[1,1].set_xlim(0, r[-1])\n",
    "axs[1,1].set_xlabel('r (Å)', fontsize = 16)\n",
    "axs[1,1].set_ylabel('RMSE', fontsize = 16)\n",
    "axs[1,1].hlines(0.03,xmin=rmin,xmax=rmax,color='k',linestyle='dashed')\n",
    "axs[1,1].text(13.6, 0.095, '(d)', fontsize = 16)\n",
    "# plt.savefig('rmse', dpi = 600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2937bef-a7c4-4b12-be85-d7b03ae4a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in torch.argsort(err)[-15:]:\n",
    "    plt.plot(r,GPOut.T[i],label=\"LGP Model\")\n",
    "    plt.plot(r,rdfs_test_bruh[i],label=\"True\")\n",
    "    plt.title(xs_test[i])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4dcdc2",
   "metadata": {},
   "source": [
    "## Timing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a4866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccce896",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    input_dict = load(open('training_data/ClassicGPTimesRDF', 'rb'))\n",
    "    evaluationTimesClassic = input_dict['evaluationTimesClassic']\n",
    "    inversionTimesClassic = input_dict['inversionTimesClassic']\n",
    "\n",
    "    print(\"Loaded previously computed times..\")\n",
    "    \n",
    "    print(\"Average inversion time for classic GP:\", np.mean(inversionTimesClassic))\n",
    "    print()\n",
    "\n",
    "    print(\"Average evaluation time for classic GP:\", np.mean(evaluationTimesClassic))\n",
    "    print()\n",
    "    \n",
    "# Get time taken for regualar GP\n",
    "except:\n",
    "    evaluationTimesClassic = [] \n",
    "    inversionTimesClassic = []\n",
    "    N_trialsClassic = 20\n",
    "    \n",
    "    print(\"Timing Kdd inversion...\")\n",
    "    \n",
    "    for n in range(N_trialsClassic):\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        # No need to apply good hps here, we only care about the time\n",
    "        KddClassic = se_kernel(XdClassic,XdClassic,torch.ones(len(XdClassic[0])),1) + 2*torch.eye(len(XdClassic))\n",
    "        KddInvClassic = torch.linalg.inv(KddClassic)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        \n",
    "        inversionTimesClassic.append(t2-t1)\n",
    "    print(\"Average inversion time for classic GP:\", np.mean(inversionTimesClassic))\n",
    "    print()\n",
    "    \n",
    "    print(\"Timing surrogate evaluation...\")\n",
    "\n",
    "    Xi = XdClassic[:len(r)]\n",
    "    \n",
    "    for n in range(N_trialsClassic):\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        μ = surrogate(Xi,XdClassic,torch.ones(len(XdClassic[0])),1,yClassic,KddInvClassic)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        \n",
    "        evaluationTimesClassic.append(t2-t1)\n",
    "        \n",
    "    print(\"Average evaluation time for classic GP:\", np.mean(evaluationTimesClassic))\n",
    "    print()\n",
    "\n",
    "    from pickle import dump\n",
    "    output_dict = dict(evaluationTimesClassic = evaluationTimesClassic, inversionTimesClassic = inversionTimesClassic)\n",
    "    dump(output_dict, open('training_data/ClassicGPTimesRDF', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_surrogate2(Xi, Xd, l, width, y, KddInv, μd):\n",
    "    \"\"\"\n",
    "    Computes the subset gaussian process estimate of the structure factor given a set of pair potential parameters. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Xi: Tensor [M,D]\n",
    "        Feature vector for M samples with D dimensions each. This corresponds to the points you wish to infer.\n",
    "    \n",
    "    Xd: Tensor [M,D]\n",
    "        Feature vector for N samples with D dimensions each. This corresponds to the points you trained on.\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter.\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "        \n",
    "    KddInv: Tensor [N,N]\n",
    "        This is the inverted kernel matrix of the training set Xd\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    μ: Tensor [η,N]\n",
    "        The mean estimate for each g(r) at each potential. The g(r)'s are organized in vertical lines where the column\n",
    "        dimension indexes the potential parameters. \n",
    "        \n",
    "    \"\"\"\n",
    "    Kid = se_kernel(Xi, Xd, l, width)\n",
    "    return (μ +(Kid @ KddInv) @ (y-μd)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    input_dict = load(open('training_data/SubsetGPTimesRDF', 'rb'))\n",
    "    evaluationTimesSubset = input_dict['evaluationTimesSubset']\n",
    "    inversionTimesSubset = input_dict['inversionTimesSubset']\n",
    "\n",
    "    print(\"Loaded previously computed times..\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Average inversion time for subset GP:\", np.mean(inversionTimesSubset))\n",
    "    print()\n",
    "\n",
    "    print(\"Average evaluation time for subset GP:\", np.mean(evaluationTimesSubset))\n",
    "    print()\n",
    "\n",
    "# Get time taken for subset GP\n",
    "except:\n",
    "    \n",
    "    evaluationTimesSubset = [] \n",
    "    inversionTimesSubset = []\n",
    "    \n",
    "    N_trials = 1000\n",
    "    \n",
    "    print(\"Timing Kdd inversion...\")\n",
    "    for n in range(N_trials):\n",
    "        \n",
    "        t1 = time.time()\n",
    "\n",
    "        KddSubset = se_kernel(Xd,Xd,l,w) + σn*torch.eye(len(Xd))\n",
    "        KddInvSubset = torch.linalg.inv(KddSubset)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        \n",
    "        inversionTimesSubset.append(t2-t1)\n",
    "    print(\"Average inversion time for subset GP:\", np.mean(inversionTimesSubset))\n",
    "    print()\n",
    "\n",
    "    XiSubset = torch.tensor([[Xd[0][0],Xd[0][1],Xd[0][2]]])\n",
    "    \n",
    "    print(\"Timing surrogate evaluation...\")\n",
    "    for n in range(N_trials):\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        μ = local_surrogate2(XiSubset,Xd,l,w,y,KddInvSubset,μd)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        \n",
    "        evaluationTimesSubset.append(t2-t1)\n",
    "        \n",
    "    \n",
    "    print(\"Average evaluation time for subset GP:\", np.mean(evaluationTimesSubset))\n",
    "    print()\n",
    "\n",
    "    from pickle import dump\n",
    "    output_dict = dict(evaluationTimesSubset = evaluationTimesSubset, inversionTimesSubset = inversionTimesSubset)\n",
    "    dump(output_dict, open('training_data/SubsetGPTimesRDF', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646eb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "numMCMCSamps = 100_000\n",
    "numGridSamps = 50**4\n",
    "# Note these assume no overhead. \n",
    "print(\"Expected time for model calls during MCMC with subset: \" + str(np.round(numMCMCSamps*np.mean(evaluationTimesSubset)/60,4)) +  \" Mins\")\n",
    "print(\"Expected time for model calls during MCMC with regular gp: \" + str(np.round(numMCMCSamps*np.mean(evaluationTimesClassic)/60,4)) +  \" Mins\")\n",
    "print()\n",
    "print(\"Expected time for model calls for grid with subset: \" + str(np.round(numGridSamps*np.mean(evaluationTimesSubset)/60/60,4)) +  \" Hours\")\n",
    "print(\"Expected time for model calls for grid with regular gp: \" + str(np.round(numGridSamps*np.mean(evaluationTimesClassic)/60/60,4)) + \" Hours\")\n",
    "print()\n",
    "print(\"Evaluation Speed up Classic vs Subset:\", np.mean(evaluationTimesClassic)/np.mean(evaluationTimesSubset))\n",
    "print(\"Inversion Speed up Classic vs Subset:\", np.mean(inversionTimesClassic)/np.mean(inversionTimesSubset))\n",
    "print(\"Evaluation Speed up Classic vs Sim:\", 1251 /np.mean(evaluationTimesClassic))\n",
    "print(\"Evaluation Speed up Subset vs Sim:\", 1251 /np.mean(evaluationTimesSubset))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f1fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
