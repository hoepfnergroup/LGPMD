{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945236e3",
   "metadata": {},
   "source": [
    "# Building Local Gaussian Process Surrogate Models from Scratch\n",
    "\n",
    "In this notebook, we outline how to implement local Gaussian process (LGP) surrogate models for complex experimental observables from scratch. This code can be customized to any existing experimental data / training set. LGP surrogate models are a fast and accurate machine learning model that can approximate the outputs of an expensive function (e.g. a molecular simulation) with uncertainty quantification (UQ). UQ alleviates the 'black box' nature of machine learning and allows us to directly quantify parameter, model, and predictive uncertainties. This uncertainty naturally allows us to perform on-the-fly learning, model validation, and parameter sensitivity analysis.\n",
    "\n",
    "The code provided below is designed to be a gentle introduction to LGP surrogate models with detailed instructions on how to implement LGP surrogates for any application. We will examine a LGP surrogate model of radial distribution functions in liquid Ne (described in the paper), but constructing your own surrogate model is as simple as uploading an experimental observable and molecular simulation training set and running them through our prebuilt functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ce094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Math Packages\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import minimize\n",
    "import time as time\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting Packages\n",
    "import matplotlib.pyplot as plt  \n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2030a0",
   "metadata": {},
   "source": [
    "## Constructing a Training Dataset\n",
    "\n",
    "The first step in building any machine learning surrogate model is generating a training set. The training set represents \"observations\" from your model at many combinations of parameters. Ideally, the training set will span the parameter space of interest and be dense enough that the machine learning model can learn the patterns and relationships in the data to a high level of accuracy.  \n",
    "\n",
    "For a single observable, $S(r_k)$, of data with many independent variables (spectra, scattering pattern, etc) the training set matrix for a local Gaussian process, $\\mathbf{\\hat{X'}}$, is an ($N$ $\\times$ dim($\\boldsymbol{\\theta}$)) matrix, \n",
    "\n",
    "\\begin{equation}\\label{eq:subsurrogate_training}\n",
    "    \\mathbf{\\hat{X'}} = \n",
    "        \\begin{bmatrix}\n",
    "        \\theta_{1,1} & \\theta_{2,1} & ...\\\\\n",
    "        \\theta_{1,2} & \\theta_{2,2} & ...\\\\\n",
    "        \\vdots & \\vdots & \\vdots\\\\\n",
    "        \\theta_{1,N} & \\theta_{2,N} & ...\\\\\n",
    "        \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "where \\theta_{i,j} is the value of the $i^{th}$ model parameter $i = 1, ..., N_{params}$ for training sample $j = 1, ..., N$. This training set matrix is just row after row of training parameters appended together.\n",
    "\n",
    "The training set observations, $\\mathbf{\\hat{Y'}}_k$, is a ($N$ $\\times$ 1) column vector of the QOIs from the training set at $r_k$,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\hat{Y}'}_k = [S(\\boldsymbol{\\theta}_1,r_k), ..., S(\\boldsymbol{\\theta}_N,r_k)]^T\n",
    "\\end{equation}\n",
    "\n",
    "where the $k$ indexes over independent variables. Therefore, all we do is take the value of $r_k$ calculated from the training simulation and make a vector of these quantities in the same order as the training set data. \n",
    "\n",
    "Below, we ran 480 molecular simulations for a ($\\lambda$-6) Mie fluid, which has three parameters: $\\lambda$, the repulsive exponent that describes the deformability of a particle in a collision, $\\sigma$ the effective collision diameter of the particle, and $\\epsilon$ the dispersive attraction or \"well-depth\" of the potential. We used molecular dynamics (MD) to calculate a radial distribution function with 73 independent variables. For now, we choose a single independent variable to model,  which gives a training set matrix of size $(480 \\times 3)$ and a training set observation vector with dimensions $(480 x 1)$. Later, we will compute a surrogate model over all 73 points independently, but for now we consider only a single point in the RDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c76626bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Matrix Shape:        (480, 3)\n",
      "Training Set Observations Shape:  (480,)\n"
     ]
    }
   ],
   "source": [
    "# Import the training data generated by 1_sample_gen.ipynb\n",
    "rmin = 0.0118331810091873\n",
    "rmax = 15.512161254882812\n",
    "rnum = 73\n",
    "\n",
    "from pickle import dump, load\n",
    "\n",
    "input_dict = load(open('training_data/samples.p', 'rb'))\n",
    "xd = input_dict['xs']\n",
    "\n",
    "input_dict = load(open('training_data/training_rdf.p', 'rb'))\n",
    "r_raw = input_dict['r']\n",
    "model_rdf_raw = input_dict['model_rdf']\n",
    "\n",
    "model_rdf = torch.zeros(len(model_rdf_raw),rnum)\n",
    "r = np.linspace(rmin,rmax,rnum)\n",
    "\n",
    "# We can interpolate this data to have the same resolution as the experiment.\n",
    "for i in range(len(model_rdf_raw)):\n",
    "    rdf_i = interpolate.splrep(r_raw, model_rdf_raw[i], s=0)\n",
    "    model_rdf[i] = torch.from_numpy(interpolate.splev(r, rdf_i, der=0))\n",
    "    \n",
    "print('Training Set Matrix Shape:       '      , np.shape(xd))                 #total training set\n",
    "print('Training Set Observations Shape: ', np.shape(np.array(model_rdf[:,0]))) #single observation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c3f93",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "A kernel, or covariance function, completely specifies a Gaussian process. The kernel is highly specific to a given problem and therefore requires careful consideration before computing Gaussian process expectations. In general, the kernel can enforce function properties like continuity and differentiability, general behavior (periodicity, positivity), and symmetry. \n",
    "\n",
    "In this code, we use the squared-exponential kernel because it enforces continuity and infinite differentiability with only two hyperparameters. The squared-exponential kernel has the general form,\n",
    "\n",
    "$$\n",
    "    K(x, x') = \\sigma^2 \\exp \\bigg(-\\frac{(x - x')^2}{2\\ell^2}\\bigg)\n",
    "$$\n",
    "\n",
    "where $\\sigma^2$ is a scaling factor that represents the variance of the function and $\\ell$ determines the length scale of fluctuations in the function. \n",
    "\n",
    "There are many resources available to choose the correct kernel for a given application. We recommend the Kernel Cookbook https://www.cs.toronto.edu/~duvenaud/cookbook/ or the textbook, Gaussian Processes by Rasmussen and Williams (2006). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef0f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_kernel(x1, x2, l, width):\n",
    "    \"\"\"\n",
    "    Computes the squared exponential kernel between the tensors x and y with hyper-parameters l and width.\n",
    "    N corresponds to the number of samples and D corresponds to the number of dimensions of the input function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x1: Tensor [N,D]\n",
    "        Feature vector for N samples with D dimensions each\n",
    "    \n",
    "    x2: Tensor [N,D]\n",
    "        Feature vector for N samples with D dimensions each\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    \n",
    "    K: Tensor [N, N]\n",
    "        Kernel matrix\n",
    "        \n",
    "    \"\"\"\n",
    "    K = width**2 * torch.exp(-(torch.cdist(x1/l,x2/l,p=2)**2)/2)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4407f",
   "metadata": {},
   "source": [
    "## The Local Gaussian Process Surrogate Model\n",
    "\n",
    "The time-complexity of the training-kernel matrix inversion and the matrix product can be substantially reduced by fragmenting a standard Gaussian process into a subset of Gaussian processes along the independent variables of the target quantity-of-interest (QOI). Under this construction, an individual $GP_k$ is trained to map a set of model parameters to an individual QOI,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbb{E}[GP_k] : \\boldsymbol{\\theta} \\mapsto S(r_k)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{r}$ is no longer an input parameter. The LGP surrogate model prediction for the observable at $r_k$, $S_{loc}^*(r_k)$, at a new set of parameters, $\\boldsymbol{\\theta}^*$, is just the expectation of the $k^{th}$ Gaussian process given the training set data,\n",
    "\n",
    "\\begin{equation}\\label{eq:subsurrogate}\n",
    "    S_{loc}^*(r_k) = \\mathbb{E}[\\textit{GP}_k(\\boldsymbol{\\theta}^*)] = \\mathbf{K}_{\\boldsymbol{\\theta}^*,\\mathbf{\\hat{X'}}} [\\mathbf{K}_{\\mathbf{\\hat{X'}}, \\mathbf{\\hat{X'}}} + \\sigma_{noise}^2 \\mathbf{I}]^{-1} \\mathbf{\\hat{Y'}}_k\n",
    "\\end{equation}\n",
    "\n",
    "In the following function, we take in variables created in the training set and the hyperparameters of the kernel to create a function that calculates this expectation for a single $k$. Later, we will need to loop this over the entire QOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8b5ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_surrogate(Xi, Xd, l, width, y, KddInv):\n",
    "    \"\"\"\n",
    "    Computes the subset gaussian process estimate of the structure factor given a set of pair potential parameters. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Xi: Tensor [M,D]\n",
    "        Feature vector for M samples with D dimensions each. This corresponds to the points you wish to infer.\n",
    "    \n",
    "    Xd: Tensor [M,D]\n",
    "        Feature vector for N samples with D dimensions each. This corresponds to the points you trained on.\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter.\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "        \n",
    "    KddInv: Tensor [N,N]\n",
    "        This is the inverted kernel matrix of the training set Xd\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    μ: Tensor [η,N]\n",
    "        The mean estimate for each S(q) at each potential. The S(q)'s are organized in vertical lines where the column\n",
    "        dimension indexes the potential parameters. \n",
    "        \n",
    "    \"\"\"\n",
    "    Kid = se_kernel(Xi, Xd, l, width)\n",
    "    return 1 + ((Kid @ KddInv) @ (y-1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecccb185",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58556b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aea0a0a1",
   "metadata": {},
   "source": [
    "## Validation with a Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a391b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26809907",
   "metadata": {},
   "source": [
    "## Concluding Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b6a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91932f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
