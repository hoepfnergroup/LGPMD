{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55f9e1d",
   "metadata": {},
   "source": [
    "# Gaussian Process Surrogate Modeling for Molecular Dynamics Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28fec8",
   "metadata": {},
   "source": [
    "## Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e673b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math Packages\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import minimize\n",
    "import time as time\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt  \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# Data saving packages\n",
    "from pickle import dump, load\n",
    "\n",
    "# Parallelization (AS IF WE WOULD DO THAT LUL)\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "mp.set_start_method('fork')\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a14a44f-d1d8-47fe-b738-76a9fdcb3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_kernel(x1, x2, l, width):\n",
    "    \"\"\"\n",
    "    Computes the squared exponential kernel between the tensors x and y with hyper-parameters l and width.\n",
    "    N corresponds to the number of samples and D corresponds to the number of dimensions of the input function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    x: Tensor [N,D]\n",
    "        Feature vector for N samples with D dimensions each\n",
    "    \n",
    "    y: Tensor [N,D]\n",
    "        Feature vector for N samples with D dimensions each\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "    \"\"\"\n",
    "    K = width**2 * torch.exp(-(torch.cdist(x1/l,x2/l,p=2)**2)/2)\n",
    "    return K\n",
    "\n",
    "def surrogate(Xi, Xd, l, width, y, KddInv):\n",
    "    \"\"\"\n",
    "    Computes the gaussian process estimate of the structure factor given a set of pair potential parameters. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Xi: Tensor [η*M,D]\n",
    "        Feature vector for M potential samples at η r evaluations with D dimensions each. This \n",
    "        corresponds to the points you wish to infer.\n",
    "    \n",
    "    Xd: Tensor [N,D]\n",
    "        Feature vector for N samples with D dimensions each. This corresponds to the points you trained on.\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter.\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "        \n",
    "    y: Tensor [N,1]\n",
    "        Output feature vector corresponding to the Xd training set.  \n",
    "        \n",
    "    KddInv: Tensor [N,N]\n",
    "        This is the inverted kernel matrix of the training set Xd\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "       \n",
    "    μ: Tensor [η,N]\n",
    "        The mean estimate for each S(q,n,sigma,epsilon) given in Xi. \n",
    "        \n",
    "    \"\"\"\n",
    "    Kid = se_kernel(Xi, Xd, l, width)\n",
    "    return 1 +  (Kid @ KddInv @ (y-1))\n",
    "\n",
    "\n",
    "def subset_surrogate(Xi, Xd, l, width, y, KddInv):\n",
    "    \"\"\"\n",
    "    Computes the subset gaussian process estimate of the structure factor given a set of pair potential parameters. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Xi: Tensor [M,D]\n",
    "        Feature vector for M samples with D dimensions each. This corresponds to the points you wish to infer.\n",
    "    \n",
    "    Xd: Tensor [M,D]\n",
    "        Feature vector for N samples with D dimensions each. This corresponds to the points you trained on.\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter.\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "        \n",
    "    KddInv: Tensor [N,N]\n",
    "        This is the inverted kernel matrix of the training set Xd\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    μ: Tensor [η,N]\n",
    "        The mean estimate for each S(q) at each potential. The S(q)'s are organized in vertical lines where the column\n",
    "        dimension indexes the potential parameters. \n",
    "        \n",
    "    \"\"\"\n",
    "    Kid = se_kernel(Xi, Xd, l, width)\n",
    "    return 1 + ((Kid @ KddInv) @ (y-1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0617c00",
   "metadata": {},
   "source": [
    "## Importing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44418d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.492161254882813\n",
      "0.20943951023931953\n"
     ]
    }
   ],
   "source": [
    "# Import the training data generated by 1_sample_gen.ipynb\n",
    "\n",
    "rmin = 0.0118331810091873\n",
    "rmax = 15.512161254882812\n",
    "rnum = 73\n",
    "\n",
    "input_dict = load(open('training_data/samples.p', 'rb'))\n",
    "xd = input_dict['xs']\n",
    "\n",
    "input_dict = load(open('training_data/training_rdf.p', 'rb'))\n",
    "r_raw = input_dict['r']\n",
    "model_rdf_raw = input_dict['model_rdf']\n",
    "\n",
    "model_rdf = torch.zeros(len(model_rdf_raw),rnum)\n",
    "r = np.linspace(rmin,rmax,rnum)\n",
    "\n",
    "# print(\"Old length: \", len(r_raw))\n",
    "# print(\"New length: \", len(r))\n",
    "\n",
    "# We can interpolate this data to have the same .\n",
    "for i in range(len(model_rdf_raw)):\n",
    "    rdf_i = interpolate.splrep(r_raw, model_rdf_raw[i], s=0)\n",
    "    model_rdf[i] = torch.from_numpy(interpolate.splev(r, rdf_i, der=0))\n",
    "     \n",
    "#model_rdf_μ = torch.sum(model_rdf,dim=0)/len(model_rdf)\n",
    "\n",
    "#plt.plot(r,model_rdf_μ)\n",
    "#plt.show()\n",
    "\n",
    "print(rmax-0.02)\n",
    "print(np.pi/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53cdb81e-7fe3-4578-8c03-1194d778bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentalCSVFilename = 'exp_data/ne_42K_rdf_new.csv'\n",
    "\n",
    "data = pd.read_csv(experimentalCSVFilename)\n",
    "r_exp_raw = np.array(data['r'])\n",
    "rdf_exp_raw = np.array(data[' g'])\n",
    "\n",
    "rmin = 0.0118331810091873\n",
    "rmax = 15.512161254882812\n",
    "rnum = 73\n",
    "\n",
    "# Interpolalate the experimental data to make it consistent with the simulations\n",
    "r  = torch.tensor(np.linspace(rmin, rmax, num=rnum))\n",
    "rdf_exp_i = interpolate.splrep(r_exp_raw, rdf_exp_raw, s=0)\n",
    "rdf_exp = torch.from_numpy(interpolate.splev(r, rdf_exp_i, der=0))\n",
    "\n",
    "# print(\"Old length: \", len(r_exp_raw))\n",
    "# print(\"New length: \", len(r))\n",
    "\n",
    "# figure(figsize = (12,10),dpi=80)\n",
    "# plt.title(\"Experimental\")\n",
    "# plt.scatter(r_exp_raw,rdf_exp_raw,alpha=0.4)\n",
    "# plt.plot(r, rdf_exp)\n",
    "# plt.xlim(rmin,rmax)\n",
    "# plt.xlabel(\"$\\AA$\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf2c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of training set\n",
    "\n",
    "# figure(figsize = (12,10),dpi=80)\n",
    "# plt.title(\"GP Training Set\")\n",
    "# for i in range(len(model_rdf_raw)):\n",
    "#     plt.plot(r,model_rdf[i],alpha=0.4)\n",
    "# plt.xlim(rmin,rmax)\n",
    "# plt.xlabel(\"$\\AA^{-1}$\")\n",
    "# plt.show()\n",
    "\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(14, 8))\n",
    "# plt.suptitle('Potential Parameter Distributions')\n",
    "# axs[0,0].scatter(xd[:, 0], xd[:, 1],label=\"Samples\")\n",
    "# axs[0,0].set_xlabel('n')\n",
    "# axs[0,0].set_ylabel('σ')\n",
    "# axs[0,1].scatter(xd[:, 0], xd[:, 2],label=\"Samples\")\n",
    "# axs[0,1].set_xlabel('n')\n",
    "# axs[0,1].set_ylabel('ϵ')\n",
    "# axs[1,0].scatter(xd[:, 1], xd[:, 2],label=\"Samples\")\n",
    "# axs[1,0].set_xlabel('σ')\n",
    "# axs[1,0].set_ylabel('ϵ')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbfad6",
   "metadata": {},
   "source": [
    "## Basic Matricies for GP Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a469b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(xd)\n",
    "η = len(r)\n",
    "XdClassic = torch.zeros(n*η,4)\n",
    "yClassic = torch.zeros(n*η)\n",
    "\n",
    "k = 0 # Row index in Xd matrix and y vector.\n",
    "for i in range(n):\n",
    "    for j in range(η):\n",
    "        # Xd_k = (n,σ,ϵ,q)\n",
    "        XdClassic[k] = torch.tensor([xd[i][0],xd[i][1],xd[i][2],r[j]])\n",
    "        yClassic[k] = model_rdf[i][j]\n",
    "        k += 1\n",
    "        \n",
    "yClassic = torch.unsqueeze(yClassic,dim=0).transpose(0,1)\n",
    "\n",
    "# Remake the Xd matrix for the subset matrix \n",
    "XdSubset = torch.tensor(xd).float()\n",
    "ySubset = model_rdf.float()\n",
    "\n",
    "indexSubset = torch.arange(0,len(xd),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab352c1",
   "metadata": {},
   "source": [
    "## Choosing the hyper parameters based off LOO and LMLH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8a7a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_marginal_LH_subset(arr, Xd, y):\n",
    "    \"\"\"\n",
    "    Computes the log marginal likelihood of one gaussian process in a subset of gaussian \n",
    "    processes for a set of hyper parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    arr: Array \n",
    "        An array of the hyper parameters to compute the model evidence at. \n",
    "        \n",
    "    y: Tensor [N,η]\n",
    "        Output feature vector corresponding to the Xd training set.  \n",
    "        \n",
    "    Xd: Tensor [M,D]\n",
    "        Feature vector for N samples with D dimensions each. This corresponds to the points you trained on.\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "       \n",
    "    μ: Float \n",
    "        The log marginal LH for the set of hyper parameters\n",
    "        x\n",
    "    \"\"\"\n",
    "    l = torch.tensor([arr[0],arr[1],arr[2]]).float()\n",
    "    w = torch.tensor(arr[3]).float()\n",
    "    σn = torch.tensor(arr[4]).float()\n",
    "    Kdd = w**2 * torch.exp(-(torch.cdist(Xd/l,Xd/l,p=2)**2)/2) + σn*torch.eye(len(Xd))\n",
    "    sign, AbsKddLogDet = torch.slogdet(Kdd)\n",
    "    KddInv = torch.linalg.inv(Kdd)\n",
    "    out = (0.5 * (y - 1).T @ KddInv @ (y - 1)  + 0.5 * AbsKddLogDet + 0.5*len(y)*np.log(2*np.pi))\n",
    "    return out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206a6a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load previously calculated hyper parameters\n",
      "Success!!!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Attempting to load previously calculated hyper parameters\")\n",
    "    \n",
    "    from pickle import load\n",
    "    input_dict = load(open('training_data/hyperParamTrainingSubsetRDF.p', 'rb'))\n",
    "    μArrSubset = input_dict['μArrSubset']\n",
    "    logMarginalLHArrSubset = input_dict['logMarginalLHArrSubset']\n",
    "    hyperParamOptionsSubset = input_dict['hyperParamOptionsSubset']\n",
    "\n",
    "    print(\"Success!!!\")\n",
    "    \n",
    "except:\n",
    "    \n",
    "    trials = 1_000\n",
    "    hyperParamOptionsSubset = torch.zeros((trials,5))\n",
    "    \n",
    "    hyperParamOptionsSubset[:,0] = (4 - 0.5) * torch.rand(trials) + 0.5\n",
    "    hyperParamOptionsSubset[:,1] = (1.3 - 0.01) * torch.rand(trials) + 0.01\n",
    "    hyperParamOptionsSubset[:,2] = (0.1 - 0.01) * torch.rand(trials) + 0.01\n",
    "    hyperParamOptionsSubset[:,3] = (0.08 - 0.0001) * torch.rand(trials) + 0.0001\n",
    "    hyperParamOptionsSubset[:,4] = (0.01 - 0.00000001) * torch.rand(trials) + 0.00000001\n",
    "    \n",
    "    μArrSubset = torch.zeros((len(hyperParamOptionsSubset),480,len(rdf_exp)))\n",
    "    logMarginalLHArrSubset = torch.zeros((len(hyperParamOptionsSubset),len(rdf_exp)))\n",
    "    \n",
    "    for j in range(len(hyperParamOptionsSubset)):\n",
    "        \n",
    "        if j % 50 == 0:\n",
    "            print(\"Starting Iteration:\", j)\n",
    "\n",
    "        # Calculate Kdd for Subset GP with hyper parameter index j\n",
    "        arrSubset = hyperParamOptionsSubset[j]\n",
    "        lSubset = torch.tensor([arrSubset[0],arrSubset[1],arrSubset[2]]).float()\n",
    "        wSubset = torch.tensor(arrSubset[3]).float()\n",
    "        σnSubset = torch.tensor(arrSubset[4]).float()\n",
    "        KddSubset = se_kernel(XdSubset,XdSubset,lSubset,wSubset) + torch.eye(len(XdSubset))*σnSubset\n",
    "\n",
    "        μArrSubset_j = torch.zeros(480,len(rdf_exp))\n",
    "        \n",
    "        for i in range(len(xd)): \n",
    "\n",
    "            KddSubset_i = KddSubset[indexSubset[indexSubset != i]].T[indexSubset[indexSubset != i]].T\n",
    "            KddInvSubset_i = torch.linalg.inv(KddSubset_i)\n",
    "\n",
    "            # Remove the same values from y\n",
    "            ySubset_i = ySubset[indexSubset != i]\n",
    "\n",
    "            # Again for X data\n",
    "            XdSubset_i = XdSubset[indexSubset[indexSubset != i]]\n",
    "            XiSubset = XdSubset[i].unsqueeze(dim=0)\n",
    "\n",
    "            # Compute the predictions after leaving one out\n",
    "            μSubset = subset_surrogate(XiSubset,XdSubset_i,lSubset,wSubset,ySubset_i,KddInvSubset_i)\n",
    "            μArrSubset_j[i] = μSubset.T\n",
    "        \n",
    "        logMarginalLHArrSubset_j = torch.zeros(len(rdf_exp))\n",
    "        \n",
    "        for k in range(η):\n",
    "            logMarginalLHArrSubset_j[k] = - neg_log_marginal_LH_subset(arrSubset,XdSubset,torch.unsqueeze(ySubset.T[k],dim=0).T)\n",
    "            \n",
    "        logMarginalLHArrSubset[j] = logMarginalLHArrSubset_j\n",
    "        μArrSubset[j] = μArrSubset_j\n",
    "        \n",
    "    from pickle import dump\n",
    "    output_dict = dict(μArrSubset = μArrSubset, hyperParamOptionsSubset = hyperParamOptionsSubset, logMarginalLHArrSubset = logMarginalLHArrSubset)\n",
    "    dump(output_dict, open('training_data/hyperParamTrainingSubsetRDF.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af644dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper parameters corresponding to the minimum leave one out error:  tensor([3.3331e+00, 1.1413e-01, 7.7208e-02, 5.7238e-02, 2.1267e-04])\n",
      "Average error per training example per point corresponding to the minimum leave one out error:  0.2325908364770619\n",
      "LMLH of minimum LOO error: -3269756.0\n",
      "\n",
      "Hyper parameters corresponding to the maximum LMLH:  tensor([3.6515, 0.2952, 0.0621, 0.0799, 0.0095])\n",
      "Maxium LMLH sum:  -894538.875\n",
      "Leave one out error corresponding to the LMLH:  0.5639863593393264\n"
     ]
    }
   ],
   "source": [
    "# Compute the leave one out error for each parameter\n",
    "LooErr = torch.zeros(1000)\n",
    "for i in range(len(hyperParamOptionsSubset[:1000])):\n",
    "    LooErr[i] = torch.sum((μArrSubset[i] - ySubset)**2)\n",
    "# Grab the one with the minimum error \n",
    "LooIndex = torch.argmin(LooErr)\n",
    "\n",
    "print(\"Hyper parameters corresponding to the minimum leave one out error: \", hyperParamOptionsSubset[LooIndex])\n",
    "print(\"Average error per training example per point corresponding to the minimum leave one out error: \", LooErr[LooIndex].item()/len(model_rdf)/len(r))\n",
    "print(\"LMLH of minimum LOO error:\",torch.sum(logMarginalLHArrSubset,dim=1)[LooIndex].item())\n",
    "print()\n",
    "# Sum the log marginal likelihood contributions over each GP.\n",
    "# Grab the one with the largest sum, aka the largest probability\n",
    "LMLHIndex = torch.argmax(torch.sum(logMarginalLHArrSubset,dim=1))\n",
    "print(\"Hyper parameters corresponding to the maximum LMLH: \", hyperParamOptionsSubset[LMLHIndex])\n",
    "print(\"Maxium LMLH sum: \", torch.sum(logMarginalLHArrSubset,dim=1)[LMLHIndex].item())\n",
    "print(\"Leave one out error corresponding to the LMLH: \", LooErr[LMLHIndex].item()/len(model_rdf)/len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e69d930-bf78-4e5e-8228-8b8347b49d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define these variables for later use when timing and validating\n",
    "lSubset = torch.tensor([3.3331e+00, 1.1413e-01, 7.7208e-02])\n",
    "wSubset =  5.7238e-02\n",
    "σnSubset = 2.1267e-04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4dcdc2",
   "metadata": {},
   "source": [
    "## Timing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cccce896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously computed times..\n",
      "Average inversion time for classic GP: 87.47960150241852\n",
      "\n",
      "Average evaluation time for subset GP: 1.0622711539268495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    input_dict = load(open('training_data/ClassicGPTimesRDF', 'rb'))\n",
    "    evaluationTimesClassic = input_dict['evaluationTimesClassic']\n",
    "    inversionTimesClassic = input_dict['inversionTimesClassic']\n",
    "\n",
    "    print(\"Loaded previously computed times..\")\n",
    "    \n",
    "    print(\"Average inversion time for classic GP:\", np.mean(inversionTimesClassic))\n",
    "    print()\n",
    "\n",
    "    print(\"Average evaluation time for subset GP:\", np.mean(evaluationTimesClassic))\n",
    "    print()\n",
    "    \n",
    "# Get time taken for regualar GP\n",
    "except:\n",
    "    evaluationTimesClassic = [] \n",
    "    inversionTimesClassic = []\n",
    "    N_trialsClassic = 20\n",
    "    \n",
    "    print(\"Timing Kdd inversion...\")\n",
    "    \n",
    "    for n in range(N_trialsClassic):\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        # No need to apply good hps here, we only care about the time\n",
    "        KddClassic = se_kernel(XdClassic,XdClassic,torch.ones(len(XdClassic[0])),1) + 2*torch.eye(len(XdClassic))\n",
    "        KddInvClassic = torch.linalg.inv(KddClassic)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        \n",
    "        inversionTimesClassic.append(t2-t1)\n",
    "    print(\"Average inversion time for classic GP:\", np.mean(inversionTimesClassic))\n",
    "    print()\n",
    "    \n",
    "    print(\"Timing surrogate evaluation...\")\n",
    "\n",
    "    Xi = XdClassic[:len(r)]\n",
    "    \n",
    "    for n in range(N_trialsClassic):\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        μ = surrogate(Xi,XdClassic,torch.ones(len(XdClassic[0])),1,yClassic,KddInvClassic)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        \n",
    "        evaluationTimesClassic.append(t2-t1)\n",
    "        \n",
    "    print(\"Average evaluation time for classic GP:\", np.mean(evaluationTimesClassic))\n",
    "    print()\n",
    "\n",
    "    from pickle import dump\n",
    "    output_dict = dict(evaluationTimesClassic = evaluationTimesClassic, inversionTimesClassic = inversionTimesClassic)\n",
    "    dump(output_dict, open('training_data/ClassicGPTimesRDF', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe3af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously computed times..\n",
      "\n",
      "Average inversion time for subset GP: 0.02091193389892578\n",
      "\n",
      "Average evaluation time for subset GP: 0.00029948973655700685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    input_dict = load(open('training_data/SubsetGPTimesRDF', 'rb'))\n",
    "    evaluationTimesSubset = input_dict['evaluationTimesSubset']\n",
    "    inversionTimesSubset = input_dict['inversionTimesSubset']\n",
    "\n",
    "    print(\"Loaded previously computed times..\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Average inversion time for subset GP:\", np.mean(inversionTimesSubset))\n",
    "    print()\n",
    "\n",
    "    print(\"Average evaluation time for subset GP:\", np.mean(evaluationTimesSubset))\n",
    "    print()\n",
    "\n",
    "# Get time taken for subset GP\n",
    "except:\n",
    "    \n",
    "    evaluationTimesSubset = [] \n",
    "    inversionTimesSubset = []\n",
    "    \n",
    "    N_trials = 1000\n",
    "    \n",
    "    print(\"Timing Kdd inversion...\")\n",
    "    for n in range(N_trials):\n",
    "        \n",
    "        t1 = time.time()\n",
    "\n",
    "        KddSubset = se_kernel(XdSubset,XdSubset,lSubset,wSubset) + σnSubset*torch.eye(len(XdSubset))\n",
    "        KddInvSubset = torch.linalg.inv(KddSubset)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        \n",
    "        inversionTimesSubset.append(t2-t1)\n",
    "    print(\"Average inversion time for subset GP:\", np.mean(inversionTimesSubset))\n",
    "    print()\n",
    "\n",
    "    XiSubset = torch.tensor([[XdSubset[0][0],XdSubset[0][1],XdSubset[0][2]]])\n",
    "    \n",
    "    print(\"Timing surrogate evaluation...\")\n",
    "    for n in range(N_trials):\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        μ = subset_surrogate(XiSubset,XdSubset,lSubset,wSubset,ySubset,KddInvSubset)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        \n",
    "        evaluationTimesSubset.append(t2-t1)\n",
    "        \n",
    "    \n",
    "    print(\"Average evaluation time for subset GP:\", np.mean(evaluationTimesSubset))\n",
    "    print()\n",
    "\n",
    "    from pickle import dump\n",
    "    output_dict = dict(evaluationTimesSubset = evaluationTimesSubset, inversionTimesSubset = inversionTimesSubset)\n",
    "    dump(output_dict, open('training_data/SubsetGPTimesRDF', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "646eb9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected time for model calls during MCMC with subset: 0.0599 Mins\n",
      "Expected time for model calls during MCMC with regular gp: 212.4542 Mins\n",
      "\n",
      "Expected time for model calls for grid with subset: 0.5199 Hours\n",
      "Expected time for model calls for grid with regular gp: 1844.2208 Hours\n",
      "\n",
      "Evaluation Speed up Classic vs Subset: 3546.9367536227733\n",
      "Inversion Speed up Classic vs Subset: 4183.238237325923\n",
      "Evaluation Speed up Classic vs Sim: 81.19244058446802\n",
      "Evaluation Speed up Subset vs Sim: 287984.4516253829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numMCMCSamps = 12_000\n",
    "numGridSamps = 50**4\n",
    "# Note these assume no overhead. \n",
    "print(\"Expected time for model calls during MCMC with subset: \" + str(np.round(numMCMCSamps*np.mean(evaluationTimesSubset)/60,4)) +  \" Mins\")\n",
    "print(\"Expected time for model calls during MCMC with regular gp: \" + str(np.round(numMCMCSamps*np.mean(evaluationTimesClassic)/60,4)) +  \" Mins\")\n",
    "print()\n",
    "print(\"Expected time for model calls for grid with subset: \" + str(np.round(numGridSamps*np.mean(evaluationTimesSubset)/60/60,4)) +  \" Hours\")\n",
    "print(\"Expected time for model calls for grid with regular gp: \" + str(np.round(numGridSamps*np.mean(evaluationTimesClassic)/60/60,4)) + \" Hours\")\n",
    "print()\n",
    "print(\"Evaluation Speed up Classic vs Subset:\", np.mean(evaluationTimesClassic)/np.mean(evaluationTimesSubset))\n",
    "print(\"Inversion Speed up Classic vs Subset:\", np.mean(inversionTimesClassic)/np.mean(inversionTimesSubset))\n",
    "print(\"Evaluation Speed up Classic vs Sim:\", 86.2483875498 /np.mean(evaluationTimesClassic))\n",
    "print(\"Evaluation Speed up Subset vs Sim:\", 86.2483875498 /np.mean(evaluationTimesSubset))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
