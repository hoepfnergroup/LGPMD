{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945236e3",
   "metadata": {},
   "source": [
    "# Building Local Gaussian Process Surrogate Models from Scratch\n",
    "\n",
    "In this notebook, we outline how to implement local Gaussian process (LGP) surrogate models for complex experimental observables from scratch. This code can be customized to any existing experimental data / training set. LGP surrogate models are a fast and accurate machine learning model that can approximate the outputs of an expensive function (e.g. a molecular simulation) with uncertainty quantification (UQ). UQ alleviates the 'black box' nature of machine learning and allows us to directly quantify parameter, model, and predictive uncertainties. This uncertainty naturally allows us to perform on-the-fly learning, model validation, and parameter sensitivity analysis.\n",
    "\n",
    "The code provided below is designed to be a gentle introduction to LGP surrogate models with detailed instructions on how to implement LGP surrogates for any application. We will examine a LGP surrogate model of radial distribution functions in liquid Ne (described in the paper), but constructing your own surrogate model is as simple as uploading an experimental observable and molecular simulation training set and running them through our prebuilt functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Math Packages\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "from scipy import interpolate\n",
    "\n",
    "# Plotting Packages\n",
    "import matplotlib.pyplot as plt  \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# Parallelization\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "from dask import config as cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2030a0",
   "metadata": {},
   "source": [
    "## Constructing a Training Dataset\n",
    "\n",
    "The first step in building any supervised machine learning surrogate model is generating a training set. The training set represents \"observations\" from your model at many combinations of parameters. Ideally, the training set will span the parameter space of interest and be dense enough that the machine learning model can learn the patterns and relationships in the data to a high level of accuracy.  \n",
    "\n",
    "For a single observable, $S(r_k)$, of data with many independent variables (spectra, scattering pattern, etc) the training set matrix for a local Gaussian process, $\\mathbf{\\hat{X'}}$, is an ($N$ $\\times$ dim($\\boldsymbol{\\theta}$)) matrix, \n",
    "\n",
    "\\begin{equation}\\label{eq:subsurrogate_training}\n",
    "    \\mathbf{\\hat{X'}} = \n",
    "        \\begin{bmatrix}\n",
    "        \\theta_{1,1} & \\theta_{2,1} & ...\\\\\n",
    "        \\theta_{1,2} & \\theta_{2,2} & ...\\\\\n",
    "        \\vdots & \\vdots & \\vdots\\\\\n",
    "        \\theta_{1,N} & \\theta_{2,N} & ...\\\\\n",
    "        \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\theta_{i,j}$ is the value of the $i^{th}$ model parameter $i = 1, ..., N_{params}$ for training sample $j = 1, ..., N$. This training set matrix is just row after row of training parameters appended together.\n",
    "\n",
    "The training set observations, $\\mathbf{\\hat{Y'}}_k$, is a ($N$ $\\times$ 1) column vector of the QOIs from the training set at $r_k$,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\hat{Y}'}_k = [S(\\boldsymbol{\\theta}_1,r_k), ..., S(\\boldsymbol{\\theta}_N,r_k)]^T\n",
    "\\end{equation}\n",
    "\n",
    "where the $k$ indexes over independent variables. Therefore, all we do is take the value of $r_k$ calculated from the training simulation and make a vector of these quantities in the same order as the training set data. \n",
    "\n",
    "Below, we ran 480 molecular simulations for a ($\\lambda$-6) Mie fluid, which has three parameters: $\\lambda$, the repulsive exponent that describes the deformability of a particle in a collision, $\\sigma$ the effective collision diameter of the particle, and $\\epsilon$ the dispersive attraction or \"well-depth\" of the potential. We used molecular dynamics (MD) to calculate a radial distribution function with 73 independent variables. For now, we choose a single independent variable to model,  which gives a training set matrix of size $(480 \\times 3)$ and a training set observation vector with dimensions $(480 x 73)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76626bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training data generated by 1_sample_gen.ipynb\n",
    "rmin = 0.0118331810091873\n",
    "rmax = 15.512161254882812\n",
    "rnum = 73\n",
    "\n",
    "from pickle import dump, load\n",
    "\n",
    "input_dict = load(open('training_data/samples.p', 'rb'))\n",
    "xd = input_dict['xs']\n",
    "\n",
    "input_dict = load(open('training_data/training_rdf.p', 'rb'))\n",
    "r_raw = input_dict['r']\n",
    "model_rdf_raw = input_dict['model_rdf']\n",
    "\n",
    "model_rdf = torch.zeros(len(model_rdf_raw),rnum)\n",
    "r = np.linspace(rmin,rmax,rnum)\n",
    "\n",
    "# We can interpolate this data to have the same resolution as the experiment.\n",
    "for i in range(len(model_rdf_raw)):\n",
    "    rdf_i = interpolate.splrep(r_raw, model_rdf_raw[i], s=0)\n",
    "    model_rdf[i] = torch.from_numpy(interpolate.splev(r, rdf_i, der=0))\n",
    "    \n",
    "print('Training Set Matrix Shape:       '      , np.shape(xd))            # total training set\n",
    "print('Training Set Observations Shape: ', np.shape(np.array(model_rdf))) # array of independent observations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c3f93",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "A kernel, or covariance function, completely specifies a Gaussian process prior. The kernel is highly specific to a given problem and therefore requires careful consideration before computing Gaussian process expectations. In general, the kernel can enforce function properties like continuity and differentiability, general behavior (periodicity, positivity), and symmetry. \n",
    "\n",
    "The kernel matrix, $\\mathbf{K}$, quantifies the relatedness between input parameters and can be selected based on prior knowledge of the physical system. A standard kernel for physics-based applications is the squared-exponential (or radial basis function) since the resulting GP is infinitely differentiable, smooth, continuous, and has an analytical Fourier transform. The squared-exponential kernel function between input points $(\\boldsymbol{\\theta}_m,\n",
    "r_m)$ and $(\\boldsymbol{\\theta}_n, r_n)$ is given by,\n",
    "\n",
    "$$\n",
    "    K_{mn} = \\alpha^2 \\exp\\bigg(-\\frac{(r_{m} - r_{n})^2}{2\\ell_{r}^2} - \\sum_{o=1}^{\\text{dim}(\\boldsymbol{\\theta})}\\frac{(\\theta_{o,m} - \\theta_{o,n})^2}{2\\ell_{\\theta_o}^2} \\bigg)\n",
    "$$\n",
    "\n",
    "\\noindent where $r$ indexes over the independent variables of the system, $o$ indexes over the model parameters of the system, and the hyperparameters $\\alpha^2$ and $\\ell_A$ are the kernel variance and correlation length scale of model parameter $A$, respectively. For a local Gaussian process, the independent variables are eliminated from the kernel matrix, giving a squared-exponential kernel of the form,\n",
    "\n",
    "\n",
    "$$\n",
    "    K_{mn} = \\alpha^2 \\exp\\bigg( - \\sum_{o=1}^{\\text{dim}(\\boldsymbol{\\theta})}\\frac{(\\theta_{o,m} - \\theta_{o,n})^2}{2\\ell_{\\theta_o}^2} \\bigg)\n",
    "$$\n",
    "\n",
    "where $\\sigma^2 \\in \\mathbb{R}$ is a scaling factor that represents the variance of the function and $\\ell \\in \\mathbb{R}^{\\text{dim}(\\theta)}$ determines the length scale of fluctuations in the function. \n",
    "\n",
    "There are many resources available to choose the correct kernel for a given application. We recommend the Kernel Cookbook https://www.cs.toronto.edu/~duvenaud/cookbook/ or the textbook, Gaussian Processes by Rasmussen and Williams (2006). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef0f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_kernel(x1, x2, l, width):\n",
    "    \"\"\"\n",
    "    Computes the squared exponential kernel between the tensors x and y with hyper-parameters l and width.\n",
    "    N corresponds to the number of samples and D corresponds to the number of dimensions of the input function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x1: Tensor [N,D]\n",
    "        Feature vector for N samples with D dimensions each\n",
    "    \n",
    "    x2: Tensor [N,D]\n",
    "        Feature vector for N samples with D dimensions each\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    \n",
    "    K: Tensor [N, N]\n",
    "        Kernel matrix\n",
    "        \n",
    "    \"\"\"\n",
    "    K = width**2 * torch.exp(-(torch.cdist(x1/l,x2/l,p=2)**2)/2)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4407f",
   "metadata": {},
   "source": [
    "## The Local Gaussian Process Surrogate Model\n",
    "\n",
    "The time-complexity of the training-kernel matrix inversion and the matrix product can be substantially reduced by fragmenting a standard Gaussian process into a subset of Gaussian processes along the independent variables of the target quantity-of-interest (QOI). Under this construction, an individual $GP_k$ is trained to map a set of model parameters to an individual QOI,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbb{E}[GP_k] : \\boldsymbol{\\theta} \\mapsto S(r_k)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{r}$ is no longer an input parameter. The LGP surrogate model prediction for the observable at $r_k$, $S_{loc}^*(r_k)$, at a new set of parameters, $\\boldsymbol{\\theta}^*$, is just the expectation of the $k^{th}$ Gaussian process given the training set data,\n",
    "\n",
    "\\begin{equation}\\label{eq:subsurrogate}\n",
    "    S_{loc}^*(r_k) = \\mathbb{E}[\\textit{GP}_k(\\boldsymbol{\\theta}^*)] = \\mathbf{K}_{\\boldsymbol{\\theta}^*,\\mathbf{\\hat{X'}}} [\\mathbf{K}_{\\mathbf{\\hat{X'}}, \\mathbf{\\hat{X'}}} + \\sigma_{noise}^2 \\mathbf{I}]^{-1} \\mathbf{\\hat{Y'}}_k\n",
    "\\end{equation}\n",
    "\n",
    "In the following function, we take in variables created in the training set and the hyperparameters of the kernel to create a function that calculates this expectation. If we want to call the subset of Gaussian processes at every indpendent input with the same $\\theta^*$ we can compute,\n",
    "\n",
    "\\begin{equation}\n",
    "    [S_{loc}^*(r_1),...,S_{loc}^*(r_\\eta)] = \\mathbf{K}_{\\boldsymbol{\\theta}^*,\\mathbf{\\hat{X'}}} [\\mathbf{K}_{\\mathbf{\\hat{X'}}, \\mathbf{\\hat{X'}}} + \\sigma_{noise}^2 \\mathbf{I}]^{-1} \\mathbf{\\hat{Y'}}\n",
    "\\end{equation}\n",
    "\n",
    "### Including a GP Prior Mean\n",
    "\n",
    "The previously described equation for the local Gaussian process surrogate model has mean 0. However, in some cases, it is useful to define an explicit prior mean function for the Gaussian process. This is particularly important for surrogate modeling complex data that is non-stationary, or in other words, has changing variances, covariances, and means as a function of its independent variables. Stochastic processes of this type are also referred to as heteroskedastic stochastic processes. Notably, a Gaussian process is a stationary model and therefore will perform poorly for non-stationary in general.\n",
    "\n",
    "In the event that the data is non-stationary, a suitable mean function can be subtracted from the observable before attempting a local Gaussian process is applied to the data. The purpose of this is to make the observations stationary. In this way, $S_{loc}^*(\\mathbf{r}|\\boldsymbol{\\theta}^*)$ becomes $y_{loc}^*(\\mathbf{r}|\\boldsymbol{\\theta}^*) - \\mathbf{\\mu}_{LGP, k}^{prior}(\\boldsymbol{\\theta}^*,\\mathbf{r})$ where the prior mean function is,\n",
    "\n",
    "$$\n",
    "    \\mathbf{\\mu}_{LGP, k}^{prior} := [\\mu(\\boldsymbol{\\theta}_1,r_k), ..., \\mu(\\boldsymbol{\\theta}_N,r_k)]^T\n",
    "$$\n",
    "\n",
    "such that $\\mu(\\boldsymbol{\\theta}_j,r_k)$ is the GP prior mean for parameter $\\boldsymbol{\\theta}_j$ at $r_k$.\n",
    "Note that the selection of a prior mean can impact the quality of fit of the GP surrogate model and should reflect physically justified prior knowledge of the physical system. In this case, we use the dilute limit potential of mean force (PMF) estimated radial distribution as the prior mean so that,\n",
    "\n",
    "$$\n",
    "    \\mathbf{\\mu}_{PMF, k}^{prior}(\\boldsymbol{\\theta}_j,r_k) := g(\\boldsymbol{\\theta}_j,r_k) = \\exp{[-\\beta V(\\boldsymbol{\\theta}_j,r_k)]}\n",
    "$$\n",
    "\n",
    "where $g(\\boldsymbol{\\theta}_j,r_k)$ and $V(\\boldsymbol{\\theta}_j,r_k)$ are the analytical dilute limit RDF and ($\\lambda$-6) Mie potential for parameters $\\boldsymbol{\\theta}_j$ at $r_k$, respectively. A PMF prior mean yields physically realistic short-range ($g(r) = 0$) and long-range behavior ($g(r) \\to 1$).\n",
    "\n",
    "If you want to compare the use of different prior means, you can simply use the local_surrogate_ideal to learn a surrogate model with an ideal gas prior and local_surrogate_pmf to learn a surrogate model with the PMF prior. This code is set up to use the PMF prior, as we have found it gives more accurate surrogate models for RDFs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_surrogate_ideal(Xi, Xd, l, width, y, KddInv):\n",
    "    \"\"\"\n",
    "    Computes the local gaussian process estimate of the structure factor given a set of pair potential parameters.\n",
    "    This function assumes that the mean of y is equal to 1.  N corresponds to the number of training samples, \n",
    "    D corresponds to the number of dimensions of the input function, and M corresponds to the number of inference\n",
    "    points.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Xi: Tensor [M,D]\n",
    "        Feature vector for M samples with D dimensions each. This corresponds to the points you wish to infer.\n",
    "    \n",
    "    Xd: Tensor [N,D]\n",
    "        Feature vector for N samples with D dimensions each. This corresponds to the points you trained on.\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter.\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "        \n",
    "    KddInv: Tensor [N,N]\n",
    "        This is the inverted kernel matrix of the training set Xd.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    μ: Tensor [N,η]\n",
    "        The mean estimate for the set of local GPs.\n",
    "        \n",
    "    \"\"\"\n",
    "    Kid = se_kernel(Xi, Xd, l, width)\n",
    "    return 1 + ((Kid @ KddInv) @ (y-1)) #by choosing a mean function of 1, we are assuming an ideal gas prior\n",
    "\n",
    "def local_surrogate_pmf(Xi, Xd, l, width, y, KddInv, μd):\n",
    "    \"\"\"\n",
    "    Computes the subset gaussian process estimate of the structure factor given a set of pair potential parameters. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    Xi: Tensor [M,D]\n",
    "        Feature vector for M samples with D dimensions each. This corresponds to the points you wish to infer.\n",
    "    \n",
    "    Xd: Tensor [M,D]\n",
    "        Feature vector for N samples with D dimensions each. This corresponds to the points you trained on.\n",
    "        \n",
    "    l: Tensor [D]\n",
    "        Lengthscale hyper parameter.\n",
    "        \n",
    "    width: Float\n",
    "        Width hyper parameter\n",
    "        \n",
    "    KddInv: Tensor [N,N]\n",
    "        This is the inverted kernel matrix of the training set Xd\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    μ: Tensor [η,N]\n",
    "        The mean estimate for each g(r) at each potential. The g(r)'s are organized in vertical lines where the column\n",
    "        dimension indexes the potential parameters. \n",
    "        \n",
    "    \"\"\"\n",
    "    #compute Mie potential at given conditions\n",
    "    V = torch.stack([((n/(n-6))*((n/6)**((6)/(n-6))))*e*((s/r)**n - (s/r)**6) for n,s,e in zip(Xi[:,0],Xi[:,1],Xi[:,2])])\n",
    "    #compute dilute limit PMF RDF\n",
    "    μ = torch.exp(-V/kbT)\n",
    "    Kid = se_kernel(Xi, Xd, l, width)\n",
    "    \n",
    "    #Subtract the prior mean from the input data, then add it back in at the end of regression\n",
    "    return (μ +(Kid @ KddInv) @ (y-μd)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecccb185",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "Choosing hyperparameters for the local GP kernel is non-trivial and requires some additional considerations. In general, changing hyperparameters can change the expectation of the local GP, so it is desirable to choose a set of hyperparameters in a systematic way. Two common methods are to (1) maximize the log-marginal likelihood (model evidence) or (2) select a set of hyperparameters that minimize the leave-one-out (LOO) error. Here we implement a brute force search for hyperparameters that minimize the LOO error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6947cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will compute the LOO for the pmf prior\n",
    "def compute_loo(hyperParamOptions,j_0, j_last):\n",
    "\n",
    "    n = len(xd)\n",
    "    η = rnum\n",
    "    \n",
    "    Xd = torch.tensor(xd).float()\n",
    "    y = torch.tensor(model_rdf).float()\n",
    "    \n",
    "    index = torch.arange(0,len(xd),1)\n",
    "    \n",
    "    looμArr = torch.zeros((j_last - j_0,480,len(r))) \n",
    "    \n",
    "    for j in range(j_0,j_last,1):\n",
    "\n",
    "        # Calculate Kdd for local GP with hyper parameter index j\n",
    "        arr = hyperParamOptions[j]\n",
    "        l = torch.tensor([arr[0],arr[1],arr[2]]).float()\n",
    "        w = torch.tensor(arr[3]).float() #α parameter in the Kernel model\n",
    "        σn = torch.tensor(arr[4]).float() #σ$_{noise}$ parameter in the expectation calculation\n",
    "        Kdd = se_kernel(Xd,Xd,l,w) + torch.eye(len(Xd))*σn\n",
    "    \n",
    "        looμArr_j = torch.zeros(480,len(r))\n",
    "        \n",
    "        #Compute PMF priors over the training dataset\n",
    "        Vd = torch.stack([((n/(n-6))*((n/6)**((6)/(n-6))))*e*((s/r)**n - (s/r)**6) for n,s,e in zip(Xd[:,0],Xd[:,1],Xd[:,2])])\n",
    "        μd = torch.exp(-Vd/kbT).float()\n",
    "    \n",
    "        # Leave index i out from training and predict it using Local GP\n",
    "        for i in range(len(xd)): \n",
    "    \n",
    "            Kdd_i = Kdd[index[index != i]].T[index[index != i]].T\n",
    "            KddInv_i = torch.linalg.inv(Kdd_i)\n",
    "    \n",
    "            # Remove the same values from y\n",
    "            y_i = y[index != i]\n",
    "    \n",
    "            # Again for X data\n",
    "            Xd_i = Xd[index[index != i]]\n",
    "            μd_i = μd[index[index != i]]\n",
    "            Xi = Xd[i].unsqueeze(dim=0)\n",
    "    \n",
    "            # Compute the predictions after leaving one out\n",
    "            looμ = local_surrogate_pmf(Xi,Xd_i,l,w,y_i,KddInv_i, μd_i)\n",
    "            looμArr_j[i] = looμ\n",
    "        \n",
    "        looμArr[j - j_0] = looμArr_j\n",
    "\n",
    "    return looμArr, hyperParamOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402566b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a set of hyper parameters to compare\n",
    "trials = 1_000 \n",
    "hyperParamOptions = torch.zeros((trials,5))\n",
    "#length scale hyperparameters for model parameters (λ, σ, ϵ)\n",
    "hyperParamOptions[:,0] = (4 - 0.5) * torch.rand(trials) + 0.5\n",
    "hyperParamOptions[:,1] = (1.3 - 0.01) * torch.rand(trials) + 0.01\n",
    "hyperParamOptions[:,2] = (0.1 - 0.01) * torch.rand(trials) + 0.01\n",
    "#width parameter of the kernel, α\n",
    "hyperParamOptions[:,3] = (0.08 - 0.0001) * torch.rand(trials) + 0.0001\n",
    "#noise parameter added along the diagonal of the covariance matrix, σ_noise\n",
    "hyperParamOptions[:,4] = (0.01 - 0.00000001) * torch.rand(trials) + 0.00000001\n",
    "\n",
    "# Storage for the leave one out prediction of local GP\n",
    "looμArr = torch.zeros((len(hyperParamOptions),480,len(r))) \n",
    "\n",
    "cfg.set({'distributed.scheduler.worker-ttl': None}) # This stops dask from crying when the sims take a long time.\n",
    "client = Client(n_workers=20)\n",
    "\n",
    "# Queue up function calls into dask\n",
    "lazy_results = []\n",
    "for i in range(int(trials/50)):\n",
    "    j_0 = (i*50)\n",
    "    j_last = ((i+1)*50)\n",
    "    lazy_results.append(dask.delayed(compute_loo)(hyperParamOptions,j_0, j_last))\n",
    "\n",
    "print(\"Queued Lazy Results\")\n",
    "\n",
    "results = dask.compute(*lazy_results)\n",
    "\n",
    "print(\"Completed Calculation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab60dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for the leave one out prediction of local GP\n",
    "looμArr = torch.zeros((len(hyperParamOptions),480,len(r))) \n",
    "\n",
    "# Unfolds the dask results\n",
    "for i in range(int(len(hyperParamOptions)/50)):\n",
    "    j_0 = (i*50)\n",
    "    j_last = ((i+1)*50)\n",
    "    looμArr[j_0:j_last] = results[i][0]\n",
    "    \n",
    "# Compute the leave one out error for each parameter\n",
    "LooErr = torch.zeros(5000)\n",
    "for i in range(len(hyperParamOptions)):\n",
    "    LooErr[i] = torch.sum((looμArr[i] - model_rdf)**2)\n",
    "# Grab the one with the minimum error \n",
    "LooIndex = torch.argmin(LooErr)\n",
    "\n",
    "print(\"Hyper parameters corresponding to the minimum leave one out error: \", hyperParamOptions[LooIndex])\n",
    "print(\"Average error per training example per point corresponding to the minimum leave one out error: \", np.sqrt(LooErr[LooIndex].item()/len(model_rdf)/len(r)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0a0a1",
   "metadata": {},
   "source": [
    "## Validation with a Test Set\n",
    "\n",
    "Now that you have specified the local Gaussian process surrogate model and found appropriate hyperparameters, you can now check whether or not the surrogate model is working properly. We evaluate the model accuracy by computing the root-mean-squared-error (RMSE) between a test set of simulation data and the LGP surrogate model. More complex metrics can be used, but RMSE is sufficent for most observables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a391b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in training set data\n",
    "input_dict = load(open('testing_data/test_data.p', 'rb'))\n",
    "totalResults = input_dict['totalResults']\n",
    "totalTime = input_dict['totalTime']\n",
    "\n",
    "# Reload Samples\n",
    "input_dict = load(open('testing_data/xs_MAPTest2.p', 'rb'))\n",
    "xs_MAPTest = input_dict['xs']\n",
    "nsampsTest = len(xs_MAPTest[:,0])\n",
    "ndimsTest  = len(xs_MAPTest[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set data|\n",
    "Xd = torch.tensor(xd).float()\n",
    "y = model_rdf.float()\n",
    "\n",
    "Vd = torch.stack([((n/(n-6))*((n/6)**((6)/(n-6))))*e*((s/r)**n - (s/r)**6) for n,s,e in zip(Xd[:,0],Xd[:,1],Xd[:,2])])\n",
    "μd = torch.exp(-Vd/kbT).float() #PMF prior mean\n",
    "\n",
    "# Choose hyperparameters\n",
    "arr = [3.9268e+00, 2.4224e-01, 9.0420e-02, 5.0339e-02, 4.8384e-04]\n",
    "l = torch.tensor([arr[0],arr[1],arr[2]]).float()\n",
    "w = torch.tensor(arr[3]).float()\n",
    "σn = torch.tensor(arr[4]).float()\n",
    "\n",
    "# Compute local GP prediction over prior predictive set\n",
    "Kdd = se_kernel(Xd,Xd,l,w) + torch.eye(len(Xd))*σn\n",
    "KddInv = torch.linalg.inv(Kdd)\n",
    "\n",
    "Xi = torch.tensor(xs_MAPTest).float()\n",
    "GPOut = local_surrogate_pmf(Xi, Xd, l, w, y, KddInv,μd)\n",
    "\n",
    "from matplotlib import cm\n",
    "err = torch.zeros(160)\n",
    "for i in range(160):\n",
    "    err[i] = torch.sqrt(torch.sum((GPOut[i] - totalResults[i])**2)/73)\n",
    "\n",
    "RMSE = torch.sqrt(torch.mean((GPOut - totalResults)**2,dim=0))\n",
    "RMSE_total = torch.sqrt(torch.mean((GPOut - totalResults)**2))\n",
    "print(\"RMSE Over Test Set:\",float(RMSE_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeec220",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 9))\n",
    "axs[0,0].scatter(xs_MAPTest[:,0], err, color = 'r', alpha = 0.6, edgecolors = 'k')\n",
    "axs[0,0].set_xlabel('λ', fontsize = 16)\n",
    "axs[0,0].set_ylabel('RMSE', fontsize = 16)\n",
    "axs[0,0].text(10.8, 0.076, '(a)', fontsize = 16)\n",
    "axs[0,1].scatter(xs_MAPTest[:,1], err, color = 'r', alpha = 0.6, edgecolors = 'k')\n",
    "axs[0,1].set_xlabel('σ (Å)', fontsize = 16)\n",
    "axs[0,1].set_ylabel('RMSE', fontsize = 16)\n",
    "axs[0,1].text(2.92, 0.076, '(b)', fontsize = 16)\n",
    "axs[1,0].scatter(xs_MAPTest[:,2], err, color = 'r', alpha = 0.6, edgecolors = 'k')\n",
    "axs[1,0].set_xlabel('ϵ (kcal/mol)', fontsize = 16)\n",
    "axs[1,0].set_ylabel('RMSE', fontsize = 16)\n",
    "axs[1,0].text(0.087, 0.076, '(c)', fontsize = 16)\n",
    "axs[1,1].plot(r,RMSE, color = 'k', label = 'Mean', linestyle = '-')\n",
    "axs[1,1].set_xlim(0, r[-1])\n",
    "axs[1,1].set_xlabel('r (Å)', fontsize = 16)\n",
    "axs[1,1].set_ylabel('RMSE', fontsize = 16)\n",
    "axs[1,1].text(13.6, 0.095, '(d)', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d5d4b",
   "metadata": {},
   "source": [
    "The RMSE is smaller than the reported uncertainty in the experimental data, so we conclude that the local GP surrogate model is sufficiently accurate to model the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26809907",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "\n",
    "Local Gaussian processes are fast and accurate surrogate models for molecular simulation applications that are especially suited for experimental data with many independent variables, such as scattering patterns or electromagnetic spectra. The methods outlined in this notebook are transferable to any data type and molecular simulation method, all that you need to do is replace the training set, test set, and experimental data with the equivalent matrices specified by your own problem. As long as these matrices are specified correctly, this notebook should be sufficient to construct an LGP surrogate model for any application in physical chemistry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87bff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902535eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
